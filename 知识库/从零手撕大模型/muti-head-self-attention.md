---
题目: Multi-Head Self-Attention
更新时间: 2024-01-01
分类: 深度学习
标签: 大模型, Transformer, Attention
引言: 多头自注意力机制的原理和实现。
---

